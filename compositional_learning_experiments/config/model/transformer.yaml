model:
  name: transformer
  d_model: 128
  nhead: 16
  num_encoder_layers: 2
  num_decoder_layers: 2
  dropout: 0.0

training:
  batch_size: 256
  learning_rate: 1.0e-3
